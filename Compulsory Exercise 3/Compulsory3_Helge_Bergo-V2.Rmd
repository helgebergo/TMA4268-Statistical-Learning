---
title: "Compulsory Exercise 3"
author: "Helge Bergo"
# date: "12.10.20"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, message = FALSE, warning = FALSE, strip.white = TRUE, prompt = FALSE, cache = TRUE, size = "scriptsize", fig.width = 4, fig.height = 4, fig.align = "center")
```

```{r, echo=FALSE}
library(rmarkdown)
library(MASS)
library(tidyverse)
library(ggfortify)
library(FactoMineR)
library(factoextra)
library(randomForest)
library(gbm)
library(gam)
```


# Problem 1

<!-- In compulsory exercise 2 we used the `College` data from the `ISLR` library, where we wanted to predict Outstate.  -->

```{r, echo = FALSE}
library(ISLR)
library(keras)
set.seed(1)
College$Private = as.numeric(College$Private)
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
```

<!-- The task here is to fit densely connected neural networks using the package `keras` in order to predict `Outstate`. -->

## a) 

<!-- Preprocessing is important before we fit a neural network. Apply feature-wise normalization to the predictors (but not to the response!). -->

Using the `College` data set, the training and test data was preprocessed, by separating the response and predictors into an x-matrix and y-vector for each set, and then scaling the predictors.

```{r 1a_preprocessing}
y.train = college.train$Outstate
y.test = college.test$Outstate

x.train <- subset(college.train, select = -c(Outstate))
x.test <- subset(college.test, select = -c(Outstate))

mean <- apply(x.train, 2, mean)
std <- apply(x.train, 2, sd)

x.train <- as.array(scale(x.train, center = mean, scale = std))
x.test <- as.array(scale(x.test, center = mean, scale = std))
```


## b) 

<!-- Write down the equation which describes a network that predicts `Outstate` with 2 hidden layers and `relu` activation function with 64 units each. -->
<!-- What activation function will you choose for the output layer? -->

The equation for the network to predict `Outstate`, using an input layer with the 17 predictors and a `relu` activation function for the hidden layers is: 

\begin{equation}
\hat{y}_1({\bf x})=\beta_{01}+\sum_{m=1}^{64} \beta_{m1}\max ( \sum_{l=1}^{64} \gamma_{lm}
\cdot \max(\sum_{j=1}^{17} \alpha_{jl}x_j,0),0)
\end{equation}

The activation function chosen for the output layer was the `linear` function, since this is a regression problem. 

## c) 

<!-- **Hints**: -->
<!-- * Use the `optimizer = "rmsprop"` , `epochs=300`  and `batch_size=8`  -->
<!-- * Make sure that you are caching the results (`cache=TRUE` in the knitr options), because fitting the models takes some time, and you do not want to repeat this each time you compile your file. -->

### (i)
<!-- (i) Train the network from b) for the training data using the library `keras`; use 20% of the training data as your validation subset (1P).  -->
The network was trained using the `keras` library, using the chosen `linear` function for the output layer, and `mse` as the loss function. 
```{r 1c_training_network, cache = TRUE}
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = c(17)) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "linear")

model %>% compile(optimizer = "rmsprop", loss = "mse")

history <- model %>% fit(x.train, y.train, epochs = 300, batch_size = 8, 
                         validation_split = 0.2)
```

### (ii)
<!-- (ii) Plot the training and validation error as a function of the epochs (1P). -->
After training for 300 epochs, with 20% of the training data as the validation set, the results are plotted below. 

```{r 1cplotting, , fig.width=5, fig.height=3}
plot(history)
```

As can be seen, both the training and validation error falls very quickly the first 30 epochs, and then continue to decrease slowly throughout the training. 

### (iii)
<!-- (iii) Report the MSE of the test set and compare it with methods that you used in Compulsory 2 (1P). -->

```{r 1cscore}
score <- model %>% evaluate(x.test, y.test)
```

The final MSE after training the model for 300 epochs was `r round(score,2)`. Compared to the MSE of the methods from Compulsory 2, this is a relatively good MSE score, and compares to both lasso and forward selection. It is better than polynomial regression and smoothing splines, but both bagging and random forest beats it, scoring `r round(3.3e6,2)` and `r round(2.6e6,2)` respectively. 

## d)

<!-- Apply one of the regularization techniques you heard about in the course (easiest to use dropout or weight decay with L1/L2 norms). Does this improve the performance of the network? -->
<!-- Optional: You might try your own network architecture. -->

Both dropout and weight decay was tried out for improving the performance of the network. 

```{r 1d, cache = TRUE, fig.width=5, fig.height=3}
model_reg <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = c(17),
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dropout(0.3) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(0.3) %>%
  layer_dense(units = 1, activation = "linear")

model_reg %>% compile(optimizer = "rmsprop", loss = "mse")

history_reg <- model_reg %>% fit(x.train, y.train, epochs = 300, batch_size = 8, 
                                 validation_split = 0.2)
```


```{r 1d2, cache = TRUE, fig.width=5, fig.height=3}
plot(history_reg) 

score_reg <- model_reg %>% evaluate(x.test, y.test)
```

After implementing 30% dropout for the two hidden layers, and L2 regularization for the first hidden layer, the final MSE after training was `r round(score_reg,digits = 2)`, so notably lower than the unimproved network, but still not better than random forest, for example. 

# Problem 2 (10P)

<!-- In this problem, we will use a real dataset of individuals with the Covid-19 infection. The data were downloaded from <https://www.kaggle.com/shirmani/characteristics-corona-patients> on 30. March 2020, and have only been cleaned for the purpose of this exercise. The dataset consists of 2010 individuals and four columns,  -->

<!--   * `deceased`: if the person died of corona (1:yes, 0:no) -->

<!--   * `sex`: male/female  -->

<!--   * `age`: age of person (ranging from 2 years to 99 years old) -->

<!--   * `country`: which country the person is from (France, Japan, Korea or Indonesia) -->

<!-- Note that the conclusions we will draw here are probably not scientifically valid, because we do not have enough information about how data were collected. -->

Load your data into R using the following code:
```{r 2_loading, echo = TRUE}
id <- "1CA1RPRYqU9oTIaHfSroitnWrI6WpUeBw" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = T)
```


## a) Inspecting your data (1P)

<!-- Inspect the data by reporting __tables__ for  -->

<!--   * the number of deceased for each country,  -->
<!--   * the number of deceased for each sex, and  -->
<!--   * for each country: the number of deceased, separate for each sex.  -->


```{r corona_tables, echo = FALSE}
library(tidyverse)
d.corona_orig <- d.corona
# d.corona_tibble <- as_tibble(d.corona)
# d.corona$deceased <- as.integer(d.corona$deceased)

knitr::kable(
  d.corona %>%
    filter(deceased == 1) %>%
    count(country),
  caption = "Number of deceased per country.")

knitr::kable(
  d.corona %>%
    filter(deceased == 1) %>%
    count(sex),
  caption = "Number of deceased per sex.")

knitr::kable(
  d.corona %>%
    select(deceased, sex, country) %>%
    pivot_wider(names_from = sex,
                values_from = deceased,
                values_fn = list(deceased = sum)),
  caption = "Number of deceased per country, separated by gender.")

```


## b) Multiple choice

<!-- Answer the following multiple choice questions by using the data above to model the probability of deceased as a function of `sex`, `age` and `country` (with France as reference level; no interactions). -->

<!-- Which of the following statements are true, which false?  -->
FALSE, FALSE, , TRUE
(i) Country is not a relevant variable in the model.

(ii) The slope for indonesia has a large $p$-value, which shows that we should remove the Indonesian population from the model, as they do not fit the model as well as the Japanese population.

(iii) Increasing the age by 10 years, $x_{age}^* = x_{age}+10$, and holding all other covariates constant, the odds ratio to die increases by a factor of 1.97.

(iv) The probability to die is approximately $3.12$ larger for males than for females.

```{r}
glm_model <- d.corona %>% 
  glm(deceased ~., data = ., family = 'binomial') 

gender_ratio <- exp(glm_model$coefficients[1] + glm_model$coefficients[2]) / glm_model$coefficients[1]
gender_ratio <- exp(glm_model$coefficients[2])

age_ratio <- mean((glm_model$coefficients[1] + glm_model$coefficients[3] * 
                     seq(30,100,10)) / 
                    (glm_model$coefficients[1] + glm_model$coefficients[3] * 
                       seq(20,90,10)))

```


## c) 

<!-- Create a plot of probabilities to die of coronavirus as a function of age, separately for the two sexes and each country.  -->

<!-- Hints:  -->
<!-- * Make one plot and add lines for each country/sex.  -->
<!-- * A useful function to generate gridded data for prediction is `expand.grid()`. For example `newdata = expand.grid(sex="male",age= seq(20,100,1) ,country="France")` generates a grid for males in France over a range of ages between 20 and 100. -->

```{r}
newdat = expand.grid(age = seq(20,100,1), sex = c('male','female'), country = unique(d.corona$country))
newdat$pred = predict(glm_model, newdata = newdat, type = 'response')

d.corona %>% 
  ggplot(aes(x = age, y = deceased, colour = country, lty = sex, alpha = 0.8)) +
  geom_line(data = newdat, aes(y = pred), size = 0.8) +
  labs(title = "Corona risk versus age")  +
  theme(legend.position = "bottom", legend.box = 'vertical') +
  scale_alpha(guide = 'none') + 
  scale_color_brewer(palette = "Set1")

```

<!-- can add this for labeling lines: # geom_text(data = filter(newdat, age == 100), aes(y = pred, label = country), col = "grey50") +  -->
 
 
## d) (3P)

As a statistician working on these data, you are asked the following questions: 

Answer the questions by fitting appropriate models (1P each). 

### i)
  (i) Have males generally a higher probability to die of coronavirus than females?

```{r}
d.corona %>% 
  lm(deceased ~ sex, data = .) %>% 
  summary
```

### ii)
  (ii) Is age a greater risk factor for males than for females?

```{r}
d.corona %>% 
  lm(deceased ~ sex*age, data = .) %>% 
  summary

```

### iii)
  (iii) Is age a greater risk factor for the French population than for the Korean population?
```{r}
d.corona %>% 
  lm(deceased ~ country*age, data = .) %>% 
  summary

```


## e) Interpret your model (1P)

According to your model fitted in part b), it looks like the French population is at a much higher risk of dying from Covid-19 than the other countries. Do you trust this result? How could it be influenced by the way the data were collected? 



## f) Multiple choice (2P)

Which of the following statements are true, which false? 


Consider the classification tree below to answer:
TRUE, TRUE,  ,  

(i) The probability of dying (`deceased = 1`) is about 0.46 for a French person with age above 91. 

(ii) Age seems to be a more important predictor for mortality than sex.

Consider the LDA code and output below:

(iii) The "null rate" for misclassification is 2.24%, because this is the proportion of deaths among all cases in the dataset. No classifier should have a higher misclassification rate.

(iv) LDA is not a very useful method for this dataset, among other reasons because it does not estimate probabilities, but also because the misclassification error is too high.


```{r, eval = T, echo = F, fig.width = 6, fig.height = 5, out.width = "60%"}
library(tree)
d.corona$deceased = as.character(d.corona$deceased)
t = tree(deceased ~ age + country + sex , data = d.corona, split = "deviance",control = tree.control(2010, mincut = 5, minsize = 10, mindev = 0.005))
plot(t)
text(t, pretty = 0)
```

```{r, eval = T, echo=T}
library(MASS)
table(predict = predict(lda(deceased ~ age + sex + country, data = d.corona))$class, true = d.corona$deceased)
```



# Problem 3 (14P)
 

<!-- The `d.support` dataset (source _F. E. Harrell, Regression Modeling Strategies_) contains the total hospital costs of 9105 patients with certain diseases in American hospitals between 1989 and 1991. The different variables are -->

<!-- Variable Meaning -->
<!-- ------- ------- -->
<!-- `totcst`   Total costs  -->
<!-- `age`      Age of the patients  -->
<!-- `dzgroup` Disease group   -->
<!-- `num.co`   Number of co-morbidities -->
<!-- `edu`     Years of education -->
<!-- `scoma`   Measure for Glasgow coma scale -->
<!-- `income`  Income  -->
<!-- `race`    Rasse  -->
<!-- `meanbp`  Mean blood pressure -->
<!-- `hrt`     Heart rate -->
<!-- `resp`    Respiratory frequency -->
<!-- `temp`    Body temperature -->
<!-- `pafi`    PaO2/FiO2 proportion (blood-gas mixture) -->

<!-- Data are loaded as follows (and we reduce the number of patients to the 4960 complete cases with total costs larger than 0): -->

```{r load_d.support, echo = FALSE, eval = FALSE}
# id <- "1heRtzi8vBoBGMaM2-ivBQI5Ki3HgJTmO" # google file ID
# d.support <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = T)
# 
# # We only look at complete cases
# d.support <- d.support[complete.cases(d.support), ]
# d.support <- d.support[d.support$totcst > 0, ]
# d.support_orig <- d.support
```


```{r load_d.support_from_file, echo = FALSE}
d.support <- read.csv("~/Dropbox/NTNU/10. semester - Vår 2020/Statistisk Læring - TMA4268/Compulsory Exercises/Compulsory Exercise 3/support.csv")
d.support <- d.support[complete.cases(d.support), ]
d.support <- d.support[d.support$totcst > 0, ]
```



```{r load_d.support_from_readr, echo = FALSE, eval = FALSE}
# d.support_r <- read_csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
# d.support_r$dzgroup <- parse_factor(d.support$dzgroup, levels = NULL, na = character(), include_na = TRUE)
# d.support_r$income <- as.factor(d.support$income)
# d.support_r$race <- as.factor(d.support$race)
# d.support_r$scoma <- as.factor(d.support$scoma)

```


<!-- We would like to build models that help us to understand which predictors are mostly driving the total cost, but also models for prediction.  -->

## a) 

<!-- Before we start analysing the data, visualize the distributions of all continuous or integer variables with histograms. Suggest a transformation for the response variable `totcst` (hint: it is a _standard transformation_ that we have used earlier in the course). Important: **you should fit all models with the transformed version of the response variable `totcst` from now on. Leave all other variables untransformed.**  -->


```{r support_histograms, fig.width = 7.5, fig.height = 4.5, echo = FALSE}
par(mfrow = c(2,5))
attach(d.support)

hist(age, main = "age", breaks = 10)
hist(num.co, main = "num.co")
hist(edu, main = "edu")
hist(scoma, main = "scoma")
hist(totcst, main = "totcst")
hist(meanbp, main = "meanbp")
hist(hrt, main = "hrt")
hist(resp, main = "resp")
hist(temp, main = "temp")
hist(pafi, main = "pafi")

```


A fitting transformation of the `totcst` variable is `log(totcst)`, as shown below.

```{r totcst_transformed, echo = FALSE, fig.width = 2, fig.height = 3}
par(mfrow = c(1,1))
hist(log(totcst), main = "log(totcst)")

```


## b) (3P)

Fit a multiple linear regression model with the six covariates `age`, `temp`, `edu`, `resp`, `num.co` and `dzgroup` and the (transformed version of the) response `totcst`. 

```{r 3b}
# d.support$num.co <- as.integer(d.support$num.co)
mlr_model <- lm(log(totcst) ~ age + temp + edu + resp + num.co + dzgroup, data = d.support)

```

### (i)
<!-- (i) How much/by which factor are the total costs expected to change when a patient's age increases by 10 years, given that all other characteristica of the patient are the same? Use the transformed respose to fit the model, but report the result on the original (back-transformed) scale of the response. (1P) -->

```{r}
age_increase_cost <- exp(mean((mlr_model$coefficients[1] + mlr_model$coefficients[2] * 
                     seq(30,100,10)) / 
                    (mlr_model$coefficients[1] + mlr_model$coefficients[2] * 
                       seq(20,90,10))))

```

The avereage total costs increase by a factor of `r round(age_increase_cost,2)` when the patient's age is increased by 10 years. 

### (ii)
(ii) Do a residual analysis using the Tukey-Anscombe plot and the QQ-diagram. Are the assumptions fulfilled? (1P)

```{r}
# par(mfrow = c(2,2))
# plot(mlr_model)
library(ggfortify)
# autoplot(lm(totcst ~ age + temp + edu + resp + num.co + dzgroup, data = d.support))
# autoplot(lm(log(totcst) ~ age + temp + edu + resp + num.co + dzgroup, data = d.support))
```


### (iii)
(iii) Does the effect of age depend on the disease group? Do a formal test and report the $p$-value. (1P)

```{r}
age_disease_model <- lm(log(totcst) ~ age*dzgroup, data = d.support)
summary(age_disease_model)

mlr_model_age <- lm(log(totcst) ~ age + dzgroup, data = d.support)
mlr_model_age_intercept <- lm(log(totcst) ~ age + dzgroup + age*dzgroup, data = d.support)
# anova(mlr_model_age, mlr_model_age_intercept)

```



## c)

<!-- In order to build a more robust model for inference and prediction of the total costs, continue using ridge regression.  -->
<!-- Create a training set with 80% of the data and a test set with the remaining 20% (1P). Run cross-validation to find the largest value of $\lambda$ such that the error is within 1 standard error of the smallest $\lambda$ (1P). Report the test MSE of the ridge regression where you used the respective $\lambda$ (1P). -->

<!-- Be careful: we still use the same transformation for the response as in b) -- you should report the MSE using the transformed version of `totcst` (i.e., do **not back-transform** the MSE to the original scale). -->

The training and test set was created, and made into a data matrix, to use the `glmnet` package. 


```{r ridge_data, eval=T}
library(glmnet)
set.seed(12345)

train.ind = sample(1:nrow(d.support), 0.8 * nrow(d.support))
d.support.train = d.support[train.ind, ]
d.support.test = d.support[-train.ind, ]
x.train = model.matrix(log(totcst) ~ ., data = d.support.train)[,-1]
y.train = log(d.support.train$totcst)
x.test = model.matrix(log(totcst) ~ ., data = d.support.test)[,-1]
y.test = log(d.support.test$totcst)
```

Cross-validation was run, to find the largest $\lambda$ within 1 standard error of the smallest $\lambda$. 

```{r ridge_cv}
ridge_model = cv.glmnet(x.train, y.train, alpha = 0)
best_lambda = ridge_model$lambda.1se
```

The value of $\lambda$ was `r round(best_lambda,3)`, which was then used to find the MSE of the ridge regression. 

```{r ridge_MSE}
ridge_pred = predict(ridge_model, s = best_lambda, newx = x.test)
ridge_MSE = mean((ridge_pred - y.test)^2)
```

The final calculated MSE is `r round(ridge_MSE,3)`. 

## d) (3P)

<!-- Now assume that our sole aim is prediction. In the course you heard about  _partial least squares (PLS)_. It is a smart approach that uses the principal component regression idea, but finds the components that are best correlated with the response. -->

<!-- Proceed as follows: -->

### (i)
(i) Run a PLS regression (don't forget to scale the variables, `scale=TRUE`) (1P).

```{r pls}
library(pls)
plsr_model <- plsr(log(totcst) ~ ., data = d.support.train, scale = TRUE, validation = "CV")
validationplot(plsr_model, val.type = "MSEP")

plsr_predictions <- predict(plsr_model, d.support.test, ncomp = 4)
plsr_MSE <- mean((plsr_predictions - log(d.support.test$totcst))^2)
```

### (ii)
(ii) Choose an optimal number of principal components (PCs) using cross-validation (1P).

### (ii)
(iii)  Report the MSE of the test set when using the respective set of PCs and compare to the result from ridge regression. Conclusion? (1P)

The final calculated MSE for PLS was `r round(plsr_MSE,3)`. 


## e) (4P)

Now choose two other methods that you know from the course and try to build models with even lower test MSEs than those found so far (imagine that this is a competition where the lowest test MSE wins). Use the same training and test dataset as generated above. And remember that we are still _always_ working with the transformed version of the response variable (`totcst`). In particular, use

(i) One model that involves non-linear transformations of the covariates (e.g., splines, natural splines, polynomials etc) that are combined to a GAM (2P).

```{r spline}

```


(ii) One model/method based on regression trees (2P).

```{r}

```


Very briefly discuss or explain your choices (1-2 sentences each).





# Problem 4 (Mixed questions; 6P)


## a) 2P

We look at the following cubic regression spline model:

$$
Y = 
\left\{
\begin{array}{ll}
 \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3  + \epsilon \ , & \text{ if } \,  x\leq 1 \ , \\
 \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x-1)^3 + \epsilon  \ , & \text{ if } \,  1 < x\leq 2 \ , \\
 \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x-1)^3 + \beta_5 (x-2)^3 + \epsilon  \ , &  \text{ if }  \, x>2 \ . \\
\end{array}
\right.
$$

Write down the basis functions (1P) and the design matrix (1P) of this model.



## b) Multiple choice - 2P

Inference vs prediction: Which of the following methods are suitable when the aim of your analysis is inference? 

(i) Lasso and ridge regression
(ii) Multiple linear regression with interaction terms
(iii) Logistic regression 
(iv) Support Vector Machines

 

## c) Multiple choice - 2P

We again look at the Covid-19 dataset from Problem 2 to study some properties of the bootstrap method. Below we estimated the standard errors of the regression coefficients in the logistic regression model with `sex`, `age` and `country` as predictors using 1000 bootstrap iterations (column `std.error`). These standard errors can be compared to those that we obtain by fitting a single logistic regression model using the `glm()` function. Look at the R output below and compare the standard errors that we obtain from these two approaches (note that the `t1*` to `t6*` variables are sorted in the same way as for the `glm()` output).

```{r,eval=T,echo=F}
id <- "1CA1RPRYqU9oTIaHfSroitnWrI6WpUeBw" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```

```{r}
# library(boot)
# boot.fn <- function(data,index){
#   return(coefficients(glm(deceased ~ sex + age + country, family="binomial",data=data,subset=index)))
# }
# boot(d.corona,boot.fn,1000)
```

```{r}
# Logistic regression
# r.glm <- glm(deceased ~ sex + age + country, d.corona,family="binomial")
# summary(r.glm)$coef
```

Which of the following statements are true?


(i) There are large differences between the estimated standard errors, which indicates a problem with the bootstrap.
(ii) The differences between the estimated standard errors indicate a problem with the assumptions taken about the distribution of the estimated parameters in logistic regression.
(iii) The `glm` function leads to too small $p$-values for the differences between countries, in particular for the differences between Indonesia and France and between Japan and France.
(iv) The bootstrap relies on random sampling the same data without replacement.




# Problem 5 (Multiple and single choice questions; 11P)

## a) Multiple choice - 2P

Which of the following are techniques for regularization?

(i) Lasso
(ii) Ridge regression
(iii) Forward and backward selection
(iv) Stochastic gradient descent

 


## b) Multiple choice - 2P

Which of the following statements about principal component regression (PCR) and partial least squares (PLS) are correct?

(i) PCR involves the first principal components that are most correlated with the response.
(ii) PLS involves the first principal components that are most correlated with the response.
(iii) The idea in PLS is that we choose the principal components that explain most variation among all covariates.
(iv) The idea in PCR is that we choose the principal components that explain most variation among all covariates.

 


## c) Single choice - 1P
In ridge regression, we estimate the regression coefficients in a linear regression model by minimizing
$$
\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j-1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^p \beta_j^2 \ .
$$

What happens when we increase $\lambda$ from 0? Choose the single correct statement:

(i) The training RSS will steadily decrease.
(ii) The test RSS will steadily decrease.
(iii) The test RSS will steadily increase.
(iv) The bias will steadily increase.
(v) The variance of the estimator will steadily increase.

 

## d) Single choice - 1P

Which statement about the _curse of dimensionality_ is correct?

(i) It means that we have a bias-variance tradeoff in $K$-nearest neighbor regression, where large $K$ leads to more bias but less variance for the predictor function.
(ii) It means that the performance of the $K$-nearest neighbor classifier gets worse when the number of predictor variables $p$ is large. 
(iii) It means that the $K$-means clustering algorithm performs bad if the datapoints lie in a high-dimensional space.
(iv) It means that support vector machines with radial kernel function should be avoided, because radial kernels correspond to infinite-dimensional polynomial boundaries.
(v) It means that we should never measure too many covariates when we want to do classification.

 

## e) Single choice - 1P

Now assume you have 10 covariates, $X_1$ to $X_{10}$, each of them uniformly distributed in the interval $[0,1]$. To predict a new test observation $(X^{(0)}_1, \ldots , X^{(0)}_{10})$ in a $K$-nearest neighbor (KNN) clustering approach, we use all observations within 20% of the range closest to each of the covariates (that is, in each dimension). Which proportion of available (training) observations can you expect to use for prediction? 

(i) $1.02 \cdot 10^{-7}$
(ii) $2.0 \cdot 10^{-3}$
(iii) $0.20$
(iv) $0.04$
(v) $10^{-10}$

 


## f) Multiple choice - 2P

This example is taken from a real clinical study by _Ikeda, Matsunaga, Irabu, et al. Using vital signs to diagnose impaired
consciousness: cross sectional observational study. BMJ 2002;325:800_. Researchers investigated the use of vital signs as a screening
test to identify brain lesions in patients with impaired
consciousness. The setting was an emergency department in
Japan. The study included 529 consecutive patients that arrived with consciousness. Patients were
followed until discharge. The vital signs of systolic and diastolic
blood pressure and pulse rate were recorded on arrival. The aim of this study was to find a quick test for assessing whether the newly arrived patient suffered from a brain lesion. 
While vital signs can be measured immediately, the actual diagnosis of a brain lesion can only be determined on the basis of brain imaging and neurological examination at a later stage, thus the quick measurements of blood pressure and heart rate are important to make a quick assessment. In total, 312 patients
(59%) were diagnosed with a brain lesion. 

The performance of each vital sign (systolic blood pressure, diastolic blood pressure and heart rate) was separately evaluated as a screening test to quickly diagnose brain lesions. To assess the quality of each of these vital signs, different thresholds were taken
successively to discriminate between “negative” and
“positive” screening test result. For each vital sign and each threshold the sensitivity and
specificity were derived and used to plot a receiver operating
characteristic (ROC) curve for the vital sign (Figure 1):

\centering
<!-- ![Figure for problem 5f); taken from _P. Sedgwick, BMJ 2011;343_](AUC.png){width=50%} -->

\flushleft

Which of the following statements are true?

(i) The value of 1-specificity represents the proportion of patients without a diagnosed brain lesion identified as positive on screening.
(ii) When we use different cut-offs, sensitivity increases at the cost of lower specificity, and vice versa.
(iii) A perfect diagnostic test has an AUC of 0.5.
(iv) The vital sign that is most suitable to distinguish between patients with and without brain lesion is systolic blood pressure. 

 


## g) Multiple choice 

<!-- We study the `decathlon2` dataset from the `factoextra` package in R, where Athletes' performance during a sporting meeting was recorded. We look at 23 athletes and the results from the 10 disciplines in two competitions. Some rows of the dataset are displayed here: -->

```{r,eval=T,echo=F}
library(factoextra)
library(FactoMineR)
data("decathlon2")
decathlon2.active <- decathlon2[1:23, 1:10]
names(decathlon2.active) <- c("100m","long_jump","shot_put","high_jump","400m","110.hurdle","discus","pole_vault","javeline","1500m")
```

```{r, echo = FALSE, eval = FALSE}
decathlon2.active[c(1,3,4),]
```

<!-- From a principal component analysis we obtain the biplot given in Figure 2. -->

```{r biplot, eval=F, echo=F, fig.width=7, fig.height=7, out.width="55%", fig.cap = "Figure for question 5g)."}
r.prcomp <- prcomp(decathlon2.active, scale = T)

biplot(r.prcomp)
```

<!-- Which of the following statements are true, which false? -->

TRUE, FALSE, TRUE, TRUE 

<!-- (i) The athlete named CLAY seems to be one of the fastest 1500m runners. -->
<!-- (ii) Athletes that are good in 100m tend to be also good in long jump. -->
<!-- (iii) The first principal component has the highest loadings for 100m and long jump. -->
<!-- (iv) 110m hurdle has a very small loading for PC2. -->

 