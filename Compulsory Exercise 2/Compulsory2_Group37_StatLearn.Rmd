---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory Exercise 2: Group 37"
author: "Anders Bendiksen and Helge Bergo"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    pdf_document:
    # html_document
    #word_document: default
editor_options: 
  chunk_output_type: inline
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, message=FALSE, warning=FALSE, strip.white=TRUE, prompt=FALSE, cache = TRUE, size="scriptsize", fig.width=4, fig.height=4, fig.align = "center")
```

```{r,eval=TRUE,echo=FALSE}
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
```


# Problem 1 

## a) Ridge Regression 

<!-- Show that the ridge regression estimator is $\hat\beta_{Ridge} = (X^T X + \lambda I)^{-1} X^T y$. -->

Using $\lambda$ as a tuning parameter and $\beta$ as the ridge regression coefficients, the goal is to minimize

$$ RSS + \lambda \sum_{j=1}^{p} \beta_j^2 $$

where $\lambda$ is greater than zero, and the residual sum of squares is

$$RSS = \sum_{i=1}^n\large(y_i - \hat\beta_0 - \sum_{j=1}^p\hat\beta_jx_{ij})^2$$
Setting $\hat\beta_0=0$, in other words setting the mean to zero, the first equation can be rewritten as 

$$ (y - X\hat\beta_{Ridge})^T(y - X\hat\beta_{Ridge}) + \lambda\hat\beta_{Ridge}^T \hat\beta_{Ridge} $$

When this is differentiated with respect to $\hat\beta_{Ridge}$ and equal to zero, we get

$$ -2X^T (y-X\hat\beta_{Ridge}) + 2\lambda\hat\beta_{Ridge} = 0 $$
$$ X^T X\hat\beta_{Ridge} + \lambda\hat\beta_{Ridge} = X^Ty $$
$$ \hat\beta_{Ridge} = (X^T X + \lambda I)^{-1} X^Ty $$


## b) 

<!-- Find the expected value and the variance-covariance matrix of $\hat\beta_{Ridge}$ (1P each). -->

Since the expected value of $y = X \beta + \epsilon$ is $X\beta$, the expectation value of $\hat\beta_{Ridge}$ is

$$\text{E}[\hat\beta_{Ridge}] = (X^TX + \lambda I)^{-1}X^TE[y] $$  
$$ = (X^TX + \lambda I)^{-1}X^TX\beta $$

The variance-covariance matrix is then (since $\text{Var}[y] = \sigma^2$)

$$ \text{Var}[\hat\beta_{Ridge}] = \text{Var}[(X^TX + \lambda I)^{-1}X^Ty]$$
$$ = (X^TX + \lambda I)^{-1}X^T \text{Var}[y]((X^TX + \lambda I)^{-1}X^T)^T$$
$$ \sigma^2(X^TX + \lambda I)^{-1}X^T X  (X^TX + \lambda I)^{-1})^T $$

## c) Multiple choice

(i)   TRUE
(ii)  FALSE
(iii) FALSE
(iv)  TRUE


## d) Forward Selection

```{r}
library(ISLR)
set.seed(1)
train.ind = sample(1:nrow(College), 0.5*nrow(College))
college.train = College[train.ind,]
college.test = College[-train.ind,]
```

```{r,echo=F,eval=F}
str(College)
```

<!-- Using the out-of-state tuition (variable `Outstate`) as response, apply forward selection in order to identify a satisfactory model that uses only a subset of all the variables in the dataset in order to predict the response. -->
<!-- Choose a model according to one of the criteria that you know and briefly say why. -->

After dividing the data into a training and test set, the `regsubsets` function was used to create a forward selection model on the data, from the `leaps`-library.

```{r p2d2, eval=TRUE, echo=TRUE}
library(leaps)
regfit.fwd = regsubsets(Outstate~.,data=college.train,method="forward", nvmax = 18)
reg.summary = summary(regfit.fwd)

```

To decide on which model is best, the number of variables used in the selection was plotted against `RSS`, `Cp`, `BIC` and `adjusted R`$^2$. 

```{r p2d3, eval=TRUE, echo=TRUE, fig.width=5, fig.height=5}
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of variables",ylab="RSS",type="b")

plot(reg.summary$adjr2,xlab="Number of variables",ylab="Adjusted Rsq",type="b")
max.adjr2 = which.max(reg.summary$adjr2)
points(max.adjr2,reg.summary$adjr2[max.adjr2], col="black",cex=2,pch=20)

plot(reg.summary$cp,xlab="Number of variables",ylab="Cp",type="b")
min.cp = which.min(reg.summary$cp)
points(min.cp,reg.summary$cp[min.cp], col="black",cex=2,pch=20)

plot(reg.summary$bic,xlab="Number of variables",ylab="BIC",type="b")
min.bic = which.min(reg.summary$bic)
points(min.bic,reg.summary$bic[min.bic], col="black",cex=2,pch=20)
```

The maximum `adjusted R`$^2$ is the one with `r max.adjr2` variables, with a value of `r reg.summary$adjr2[max.adjr2]`, shown as a filled dot in the upper right plot. This is also the same number of variables as for the lowest `Cp`. However, all the plots are pretty flat after around 6 or 7 variables used, and it seems like using only 6 variables still gives a good `adjusted R`$^2$ value of `r reg.summary$adjr2[6]`, without the increased complexity of adding 7 more variables. The model is then:

```{r}
coef(regfit.fwd,6)
```

 
```{r, eval=F, echo=F}
coef.for = coef(regfit.fwd,max_point)
coef.for
```

For the MSE, the following code calculates the MSE for all the variables. 


```{r, eval=TRUE, echo=TRUE }
val.errors = rep(NA,17)
x.test = model.matrix(Outstate~.,data=college.test) # notice the -index!
for (i in 1:17) {
    coefi = coef(regfit.fwd,id=i)
    pred = x.test[,names(coefi)]%*%coefi
    val.errors[i] = mean((college.test$Outstate-pred)^2)
}

```


```{r, include=F}
# plot(sqrt(val.errors),xlab="Number of variables", ylab="Root MSE",ylim=c(1500,5000) ,pch=19,type="b")
# points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b")
# legend("topright",legend=c("Training","Validation"),col=c("black","blue"),pch=19)
```

The MSE of the model with 6 variables is then: 
```{r}
val.errors[6]
```


## e) 

<!-- Now do model selection using the same dataset as in (d) using the Lasso method. How did you select the tuning parameter $\lambda$? -->
<!-- Report the set of variables that was selected and the MSE on the test set. -->

Using the Lasso method from the `glmnet`-library, a new model was selected. 

```{r eval=T, echo=F}
library(glmnet)
x.train = model.matrix(Outstate~.,data=college.train)[,-1]
y.train = college.train$Outstate
x.test = model.matrix(Outstate~.,data=college.test)[,-1]
y.test = college.test$Outstate

lasso.model = glmnet(x.train,y.train,alpha = 1)
# plot(lasso.model)
```

To select the tuning parameter $\lambda$, cross-validation was performed, and the $\lambda$ giving the lowest MSE was selected.

```{r, fig.width=4, fig.height=4}
cv.out = cv.glmnet(x.train,y.train, alpha = 1)
best.lambda = cv.out$lambda.min
best.lambda
```

This was used on the test set, to get the MSE for the lasso. 

```{r}
lasso.pred = predict(lasso.model,s=best.lambda ,newx=x.test)
MSE.lasso = mean((lasso.pred-y.test)^2)
MSE.lasso
```

Finally, the coefficients of the model are shown here: 

```{r}
lasso.coef = predict(cv.out,type="coefficients",s=best.lambda)[1:18,]
lasso.coef

```
 

# Problem 2 

## a) Multiple choice

<!-- Which of the following statements are true, which false?  -->

<!-- (i) A regression spline of order 3 with 4 knots has 8 basis functions. -->
<!-- (ii) A regression spline with polynomials of degree $M-1$ has continuous derivatives up to order $M-2,$ but not at the knots. -->
<!-- (iii) A natural cubic spline is linear beyond the boundary knots. -->
<!-- (iv) A smoothing spline is (a shrunken version of) a natural cubic spline with knots at the values of all data points $x_i$ for $i=1,\ldots ,n$. -->

 (i) FALSE
 (ii) FALSE
 (iii) TRUE
 (iv) TRUE

## b) 

<!-- Write down the basis functions for a cubic spline with knots at the quartiles $q_1, q_2, q_3$ of variable $X$. -->

The basis representations for the cubic spline with three knots are as follows. 

\begin{equation*}
y_i = 
\begin{cases}
 \beta_{01} + \beta_{11} x_i + \beta_{21} x_i^2 + \beta_{31} x^3_i +\epsilon_i 
&\text{if} \hspace{0.5mm}-\infty\leq x \geq q_1\\
\beta_{02}+\beta_{12}x_i+\beta_{22}x_i^2+\beta_{32}x^3_i+\epsilon_i +(x-q_1)^3 
&\text{if} \hspace{5mm}q_1\leq x \geq q_2 \\
\beta_{03}+\beta_{13}x_i+\beta_{23}x_i^2+\beta_{33}x^3_i+\epsilon_i + (x-q_2)^3 
&\text{if} \hspace{5mm}q_2\leq x \geq q_3 \\
\beta_{02}+\beta_{12}x_i+\beta_{22}x_i^2 + \beta_{32}x^3_i+\epsilon_i +(x-q_3)^3  
&\text{if} \hspace{5mm}q_3\leq x \geq \infty
\end{cases}
\end{equation*}

$X$ can be expressed as a vector of components $x_i$ 
$$ X= [x_1, x_2, x_ 3, ..., x_n]$$

And $Y$ can similarly be expressed as:
$$ Y= [y_1, y_2, y_ 3, ..., y_n]$$

With $i$ being a number 1 to $n$, and $n$ being the number of components in the $X$ and $Y$ vectors. 
The basis functions are the functions or operations that are applied to $X$ in order to fit $Y$. 

\begin{equation*}
y_i = 
\begin{cases}
\beta_{01} + \beta_{11}B_1(x_i)+\beta_{21}B_2(x_i)+\beta_{31}B_3(x_i)_i+\epsilon_i 
&\text{if} \hspace{0.5mm}-\infty\leq x \geq q_1 \\
\beta_{02}+\beta_{12}B_1(x_i)+\beta_{22}B_2(x_i)+\beta_{32}B_3(x_i)+\epsilon_i +B_4(x, \zeta) 
&\text{if} \hspace{5mm}q_1\leq x \geq q_2 \\
\beta_{03}+\beta_{13}B_1/x_i)+\beta_{23}B_2(x_i)+\beta_{33}B_3(x_i)+\epsilon_i +B_4(x, \zeta) 
&\text{if} \hspace{5mm}q_2\leq x \geq q_3 \\
\beta_{02}+\beta_{12}B_1(x_i)+\beta_{22}B_2(x_i)+\beta_{32}B_3(x_i)+\epsilon_i +B_4(x, \zeta)
&\text{if} \hspace{5mm}q_3\leq x \geq \infty
\end{cases}
\end{equation*}

The basis functions; $B_1$, $B_2$, $B_3$, $B_4$ can then be expressed as:

\begin{align*}
B_1(x) &=x \\
B_2(x) &= x^2\\
B_3(x) &= x^3\\
B_4(x) &= (x-\zeta)^3
\end{align*}

```{r 2c1, echo=FALSE}
library(leaps)
for.reg = regsubsets(Outstate~., data = college.train, method = "forward")
coef.for = coef(for.reg, id = 6)
co.names =  names(coef.for)[-1]
co.names[1] = "Private" 
```

## c)

<!-- We continue with using the `College` dataset that we used in problem 1. Investigate the relationships between `Outstate` and the following 6 predictors (using the training dataset `college.train`): -->
<!-- Create some informative plots and say which of the variables seem to have a linear relationship with the response, and which might benefit from a non-linear transformation (like e.g. a spline). -->

The variables 
```{r, echo = FALSE}
co.names
```
were plotted against `Outstate` to look at the relationship between the variables. 


```{r 2c3, fig.width=6, fig.height=4, echo=FALSE}
par(mfrow=c(2,3), cex=0.5)
plot(College$Private,College$Outstate, xlab="Private", ylab="Outstate")
plot(College$Room.Board,College$Outstate, xlab="Room.Board", ylab="Outstate")
plot(College$Terminal,College$Outstate, xlab="Terminal", ylab="Outstate")
plot(College$perc.alumni,College$Outstate, xlab="perc.alumni", ylab="Outstate")
plot(College$Expend,College$Outstate, xlab="Expend", ylab="Outstate")
plot(College$Grad.Rate,College$Outstate, xlab="Grad.Rate", ylab="Outstate")

```

From these plots, it seems like `Room.board`, `perc.alumni` and `Grad.Rate` all have quite linear relationships with `Outstate`, while both `Terminal` and `Expend` seem to follow a non-linear relationship. 

## d) 

### (i) 

<!-- Fit polynomial regression models for `Outstate` with `Terminal` as the only covariate for a range of polynomial degrees ($d = 1,\ldots,10$) and plot the results. Use the training data (`college.train`) for this task. -->

The data from `college.train` was fitted with polynomial regression for the degrees $d = 1,\ldots,10$. The code and plot is shown below. 

```{r 2di}
cols = rainbow(10)
deg = 1:10
polyfunc = function(d) {
  model = lm(Outstate ~ poly(Terminal,d), data=college.train)
  lines(cbind(college.train$Terminal,model$fit)[order(college.train$Terminal),],
        col=cols[d])
  pred = predict(model, college.train)
  mean((pred - college.train$Outstate)^2)
}
plot(college.train$Terminal, college.train$Outstate, col = "gray", pch=19, 
     cex = 0.5, xlab = "Terminal", ylab = "Outstate")
MSE.poly = sapply(deg, polyfunc)
legend("topleft",legend = paste("degree = ",deg), col = cols, cex = 0.4)
```


### (ii) 
<!-- Still for the training data, choose a suitable smoothing spline model to predict `Outstate` as a function of `Expend` (again as the only covariate) and plot the fitted function into the scatterplot of `Outstate` against `Expend`. How did you choose the degrees of freedom? -->


```{r 2dii, eval=TRUE, echo=TRUE}
library(splines)
expend.range = range(college.train$Expend)
expend.grid = seq(from=expend.range[1], to=expend.range[2])
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", pch=19, 
     cex = 0.5, xlab = "Expend", ylab = "Oustate")
fit.smoothspline = smooth.spline(college.train$Expend, college.train$Outstate,cv=TRUE)
lines(fit.smoothspline)

```

The degrees of freedom was chosen using cross-validation, and the result was `r round(fit.smoothspline$df,3)`. 


### (iii) 
<!-- Report the corresponding training MSE for (i) and (ii). Did you expect that? -->

```{r, eval=TRUE, echo=TRUE}
MSE.smoothspline.train = mean((predict(fit.smoothspline, college.train$Expend)$y - 
                                 college.train$Outstate)^2)
MSE.smoothspline.train
MSE.poly
```

The MSE for the polynomial regression is much higher than the MSE for the smoothing splines, but this makes a lot of sense when looking at the initial plots from 2.c). For the `Expend` variable, it seems like the data have a clearer trend than for the `Terminal` variable, and therefore the MSE is much lower. 


```{r, include = FALSE}
# MSE.smoothspline.test = mean((predict(fit.smoothspline, college.test$Expend)$y - 
                                # college.test$Outstate)^2)
# MSE.smoothspline.test
```



# Problem 3 

## a) Multiple choice

<!-- Which of the following statements are true, which false? -->

<!-- (i) Regression trees cannot handle categorical predictors. -->
<!-- (ii) Regression and classification trees are easy to interpret. -->
<!-- (iii) The random forest approaches improves bagging, because it reduces the variance of the predictor function by decorrelating the trees. -->
<!-- (iv) The number of trees $B$ in bagging and random forests is a tuning parameter. -->

 (i) TRUE
 (ii) TRUE
 (iii) TRUE
 (iv) FALSE

## b)

<!-- Select one method from Module 8 (tree-based methods) in order to build a good model to predict `Outstate` in the `College` dataset that we used in problems 1 and 2.  Explain your choice (pros/cons?) and how you chose the tuning parameter(s). Train the model using the training data and report the MSE for the test data.  -->

In order to find the best method for the data at hand, we first considered regression trees. This was quite obvious, as the tuition is not a "classifiable" value. Next up is bagging and random forest. Random forest is probably the best choice of these, as the trees are decorrelated. We will however find the MSE of all these three methods, and compare them to each other. 

```{r, echo=FALSE}
# library(caret) 
# library(Ecdat)
library(ipred)
# library(vcd)
# library(rpart)
library(tree)
library(randomForest)
# library(Ecfun)
```


```{r, include=FALSE}
# train.ind = sample(1:nrow(College), 0.5 * nrow(College))
# college.train = College[train.ind, ]
# college.test = College[-train.ind, ]

# tree <- tree(Outstate ~ Apps + Accept + Enroll + Top10perc + Top25perc + F.Undergrad + P.Undergrad + Room.Board + Books + Personal + PhD + Terminal  + S.F.Ratio + perc.alumni + Expend + Grad.Rate, data=college.train, method="anova")
```


First, a regression tree was created. 

```{r}
tree <- tree(Outstate ~.,data=college.train, method="anova")
plot(tree, cex = 0.6)
text(tree, pretty = 0, cex = 0.6)
```

Using cross-validation, the tree with the lowest deviance was found to be the tree with a size of 8. The tree was then pruned, and the MSE was calculated for the pruned tree. 

```{r}
tree.cv <- cv.tree(tree)
tree.mindev = tree.cv$size[which.min(tree.cv$dev)]
tree.prune = prune.tree(tree, best = tree.mindev)
plot(tree.prune, cex = 0.6)
text(tree.prune, pretty = 0, cex = 0.6)

tree.predict = predict(tree.prune, newdata = college.test)
MSE.tree = mean((tree.predict - college.test$Outstate)^2)
```


```{r, include = FALSE}
plot(tree.predict, cex = 0.6)
text(tree, pretty = 0, cex = 0.6)
```

Then, doing the same using bagging, the MSE was calculated again. 

```{r }
tree.bag <- bagging(Outstate~.,data=college.train, nbagg=25)
tree.bag.predict <- predict(tree.bag, college.test)
MSE.bag = mean((tree.bag.predict - college.test$Outstate)^2)
```

Finally, a random forest was created with the dataset. 

```{r}
tree.randomForest = randomForest(Outstate~., data = college.train, mtry =
                        ncol(college.train)/3, ntree = 500,importance = TRUE)
randomForest.predict = predict(tree.randomForest, newdata = college.test)
MSE.randomForest = mean((randomForest.predict - college.test$Outstate)^2)
```

The resulting MSEs are shown for the three methods in the following table. 

\begin{table}[ht]
\centering
\begin{tabular}{l l}
\hline
\textbf{Method} & \textbf{MSE} \\
\hline
Regression tree & `r signif(MSE.tree,digits=2)` \\
Bagging  & `r signif(MSE.bag,digits=2)` \\
Random forest & `r signif(MSE.randomForest,digits=2)` \\
\hline
\end{tabular}
\end{table}

As can be seen, the MSE is lowest for the random forest model, and it is significantly lower than both the pruned regression tree, and the bagging. 
<!-- Just about 1500 USD in actual values.  -->
This is most probably because of the decorrelated trees. Bagging was better than regression trees

## c) 

<!-- Compare the results (tests MSEs) among all the methods you used in Problems 1-3. Which method perform best in terms of prediction error? Which method would you choose if the aim is to develop an interpretable model? -->

The results from Problem 1-3 are shown in the table below, in chronological order. 

\begin{table}[ht]
\centering
\begin{tabular}{l l}
\hline
\textbf{Method} & \textbf{MSE} \\
\hline
Forward selection & `r signif(val.errors[6],digits=2)` \\
Lasso & `r signif(MSE.lasso,digits=2)` \\
Polynomial regression & `r signif(MSE.poly[which.min(MSE.poly)],digits=2)` \\
Smoothing spline & `r signif(MSE.smoothspline.train,digits=2)` \\
Regression tree & `r signif(MSE.tree,digits=2)` \\
Bagging  & `r signif(MSE.bag,digits=2)` \\
Random forest & `r signif(MSE.randomForest,digits=2)` \\
\hline
\end{tabular}
\end{table}

In terms of prediction error, the best model is by far the random forest, compared to all the others. However, the random forest model is not the most interpretable model, and if that would be a goal, the pruned regression tree, lasso, or forward selection would be better models. They have a slightly higher MSE, but this might be worth it for the simplicity. All in all, the forward selection model might be the best model when interpretability is desired. 

<!-- However, if the goal is to develop an interpretable model, the model chosen would probably be the pruned regression tree, as this requires the least statistical knowledge. This has a higher MSE, but this might be a worth trade-off for the added interpretability.  -->



# Problem 4

<!-- We will use the classical data set of _diabetes_ from a population of women of Pima Indian heritage in the US, available in the R `MASS` package. The following information is available for each woman: -->

<!-- * diabetes: `0`= not present, `1`= present -->
<!-- * npreg: number of pregnancies -->
<!-- * glu: plasma glucose concentration in an oral glucose tolerance test -->
<!-- * bp: diastolic blood pressure (mmHg) -->
<!-- * skin: triceps skin fold thickness (mm) -->
<!-- * bmi: body mass index (weight in kg/(height in m)$^2$) -->
<!-- * ped: diabetes pedigree function. -->
<!-- * age: age in years -->


<!-- We will use a training set (called `d.train`) with 300 observations (200 non-diabetes and 100 diabetes cases) and a test set (called `d.test`) with 232 observations (155 non-diabetes and 77 diabetes cases). Our aim is to make a classification rule for the presence of diabetes (yes/no) based on the available data. You can load the data as follows: -->


```{r, eval = TRUE, echo = FALSE}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E" # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
d.train=d.diabetes$ctrain
d.test=d.diabetes$ctest
attach(d.train)
```


## a) Multiple choice

<!-- Start by getting to know the _training data_, by producing summaries and plots. Which of the following statements are true, which false? -->

<!--  (i) Females with high glucose levels and higher bmi seem to have a higher risk for diabetes. -->
<!--  (ii) Some women had up to 17 pregnancies. -->
<!--  (iii) BMI and triceps skin fold thickness seem to be positively correlated. -->
<!--  (iv) The distribution of the number of pregnancies per woman seems to be a bit skewed and a transformation of this variable could therefore be appropriate. -->

```{r, include = FALSE}
summary(d.train)
str(d.train)
plot(d.train$glu,d.train$bmi,col = d.train$diabetes, xlab = "Glucose", ylab = "BMI")
legend("topright",legend= c("not diabetes", "diabetes"), col = c(1,2),pch = 19)
plot(d.train$bmi,d.train$skin)
max(d.train$npreg)
hist(d.train$npreg)
```

 (i) TRUE
 (ii) TRUE
 (iii) TRUE
 (iv) TRUE

## b) 

<!-- Fit a support vector classifier (linear boundary) and a support vector machine (radial boundary) to find good functions that predict the diabetes status of a patient. Use cross-validation to find a good cost parameter (for the linear boundary) and a good combination of cost _and_ $\gamma$ parameters (for the radial boundary). Report the confusion tables and misclassification error rates for the test set in both cases. Which classifier do you prefer and why? -->
<!-- (Do not use any variable transformations or standardizations to facilitate correction). -->

<!-- **R-hints:** The response variable must be converted into a factor variable before you continue. -->

First, we convert the variables to factors, and fit a support vector classifier using the `e1071` package and the `svm`function, and cross validation to find the best cost parameter.

```{r svm.linear, eval = T, echo = T}
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
library(e1071)

svm.linear = tune(svm,diabetes~.,data=d.train,kernel="linear", 
                  ranges = list(cost=c(0.001,0.01,0.1,1,5,10,100)))
svm.linear.pred  = predict(svm.linear$best.model,d.test)
svm.linear.table = table(predict=svm.linear.pred, truth = d.test$diabetes)
svm.linear.error = sum(svm.linear.table[2:3]) / sum(svm.linear.table)
```

The confusion table and the misclassification error rate is: 

```{r}
svm.linear.table
svm.linear.error
```

Then, a support vector machine was fitted, again with cross validation, but this time to find the optimal combination of cost and $\gamma$. 

```{r svm.radial}
svm.radial = tune(svm,diabetes~.,data=d.train,kernel="radial", 
                       ranges = list(cost=c(0.001,0.01,0.1,1,5,10,100),
                                     gamma=c(0,0001, 0.001,0.01,0.1,1,5,10,100)))
svm.radial.pred  = predict(svm.radial$best.model,d.test)
svm.radial.table = table(predict=svm.radial.pred, truth = d.test$diabetes)
svm.radial.error = sum(svm.radial.table[2:3]) / sum(svm.radial.table)
```

The confusion table and the misclassification error rate is: 

```{r}
svm.radial.table
svm.radial.error
```

Comparing these two, the misclassification error rate is actually identical for the given test set, but there are some differences in the confusion matrices. There are more negative predictions in the radial model, both true and false negatives, 3 more on each. This is interesting, and shows the difference between the two types of boundaries and the impact a difference in cost and $\gamma$ gives. For the given data, the preferred model would probably be the linear one, as this is both simpler, and gives a higher number of true positives. In the case of diabetes, misclassification in the form of false positives is better than false negatives, in our opinion. 


## c) 

<!-- Compare the performance of the two classifiers from b) to _one other classification method_ that you have learned about in the course. Explain your choice and report the confusion table and misclassification error rate on the test set for your chosen method and interpret what you see. What are advantages/disadvantages of your chosen method with respect to SVMs? -->

Comparing the SVMs to a linear discriminant analysis, the following code gives a fit using LDA. 

```{r}
lda.fit = lda(diabetes~., data = d.train)
lda.pred = predict(lda.fit,d.test)
lda.table = table(predict=lda.pred$class, truth = d.test$diabetes)
lda.error = sum(lda.table[2:3]) / sum(lda.table)
lda.table
lda.error
```

As can be seen, the misclassification rate is very similar, with only one less false negative compared to the support vector classifier. The main difference between the two methods is that the SVM uses only some observations as vectors to create the separating hyperplane, while the LDA uses all observations. This makes SVM less dependant on observations far from the hyperplane, while LDA is more affected by outliers in the data. 


```{r ROCR, include = FALSE}
library(ROCR)
rocplot=function(pred, truth, ...){
  predob = prediction (pred, truth)
  perf = performance (predob , "tpr", "fpr") 
  plot(perf ,...)}

svmfit.opt = svm(diabetes~., data=d.train, kernel="radial", gamma=0.01, cost=10,decision.values=T)
fitted.train = attributes(predict(svmfit.opt,d.train,decision.values=TRUE))$decision.values

par(mfrow=c(1,2))
rocplot(fitted.train,d.train$diabetes,main="Training Data")

svmfit.flex=svm(diabetes~., data=d.train, kernel="radial", gamma=1, cost=10, decision.values=T)
fitted.flex=attributes(predict(svmfit.flex,d.train,decision.values=T))$decision.values
rocplot(fitted.flex,d.train$diabetes,add=T,col="red")

fitted.test=attributes(predict(svmfit.opt,d.test,decision.values=T))$decision.values
rocplot(fitted.test,d.test$diabetes,main="Test Data")

fitted.test.flex=attributes(predict(svmfit.flex,d.test,decision.values=T))$decision.values
rocplot(fitted.test.flex,d.test$diabetes,add=T,col="red")

```

<!-- The fact that the support vector classifierâ€™s decision rule is based only on a potentially small subset of the training observations (the support vectors) means that it is quite robust to the behavior of observations that are far away from the hyperplane. This property is distinct from some of the other classification methods that we have seen in preceding chapters, such as linear discriminant analysis. Recall that the LDA classification rule depends on the mean of all of the observations within each class, as well as the within-class covariance matrix computed using all of the observations. In contrast, logistic regression, unlike LDA, has very low sensitivity to observations far from the decision boundary. In fact we will see in Section 9.5 that the support vector classifier and logistic regression are closely related. -->


```{r, include = FALSE}
qda.fit = qda(diabetes~., data = d.train)
qda.pred = predict(qda.fit,d.test)
qda.table = table(predict=qda.pred$class, truth = d.test$diabetes)
qda.error = sum(qda.table[2:3]) / sum(qda.table)
qda.table
qda.error
```


## d) Multiple choice

<!-- Which of the following statements are true, which false?  -->

<!-- (i) Under standard conditions, the maximal margin hyperplane approach is equivalent to a linear discriminant analysis. -->
<!-- (ii) Under standard conditions, the support vector classifier is equivalent to quadratic discriminant analysis. -->
<!-- (iii) Logistic regression, LDA and support vector machines tend to perform similar when decision boundaries are linear, unless classes are linearly separable. -->
<!-- (iv) An advantage of logistic regression over SVMs is that it is easier to do feature selection and to interpret the results.  -->

 (i) FALSE
 (ii) FALSE
 (iii) TRUE
 (iv) TRUE

## e) Link to logistic regression and hinge loss.

<!-- Look at slides 71-73 of Module 9. Show that the loss function -->
<!-- $$ \log(1+\exp(-y_i f({\boldsymbol x}_i))) $$ -->

<!-- is the deviance for the $y=-1,1$ encoding in a logistic regression model.   -->
<!-- **Hint**: $f({\boldsymbol x}_i)$ corresponds to the linear predictor in the logistic regression approach. -->

Using $f({\boldsymbol x}_i)$ as corresponding to the linear predictor in the logistic regression approach,

$$ f({\boldsymbol x)}_i = \frac{e^{\beta_0 + \beta_1x_{i1} + ... + \beta_p x_{ip}}}
{1 + e^{\beta_0 + \beta_1x_{i1} + ... + \beta_p x_{ip}}} $$

the logistic regression model is on the form

$$ p_i = \frac{e^{f({\boldsymbol x}_i)}}{1 + e^{f({\boldsymbol x}_i)}}.$$

In logistic regression, the observations contribute by a weight $p_i(1-p_i)$, so the regression model can be rewritten to

$$ f({\boldsymbol x}_i) = \log(\frac{p_i}{1-p_i}) $$
This means that the loss function $$ \log(1 + exp(-y_if({\boldsymbol x}_i)) $$ is the deviance for the $y=-1,1$ encoding in a logistic regression model. 


# Problem 5 

<!-- The following dataset consists of 40 tissue samples with measurements of 1,000 genes. The first 20 tissues come from healthy patients and the remaining 20 come from a diseased patient group. The following code loads the dataset into your session with row names describing if the tissue comes from a diseased or healthy person. -->

```{r, echo = F}
library(dendextend)
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU" # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = " ")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = " ")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
GeneData <- t(GeneData)
```


## a) 

<!-- Perform hierarchical clustering with complete, single and average linkage using __both__ Euclidean distance and correlation-based distance on the dataset. Plot the dendograms. Hint: You can use `par(mfrow=c(1,3))` to plot all three dendograms on one line or `par(mfrow=c(2,3))` to plot all six together. -->
 
Performing hierarchical clustering with both correlation and euclidian distance metrics. 

```{r }
hc.complete = hclust(dist(GeneData, method = "euclidean", diag = FALSE, upper = FALSE, 
                          p = 2), method ="complete")
hc.single = hclust(dist(GeneData, method = "euclidean", diag = FALSE, upper = FALSE, 
                        p = 2), method ="single")
hc.average = hclust(dist(GeneData, method = "euclidean"), method = "average")  

dd = as.dist(cor(t(GeneData))) 

corr.comp <- hclust(dd, method ="complete")
corr.av <- hclust(dd, method ="average")
corr.single <- hclust(dd, method ="single")
```


```{r, fig.width=6, fig.height=5, echo=F}
par(mfrow=c(2,3), cex =  0.5)
plot(corr.comp, main="Complete Linkage, correlated", xlab="", sub ="")
plot(corr.single, main="Single Linkage, correlated", xlab="", sub ="")
plot(corr.av, main="Average Linkage, correlated", xlab="", sub ="")

plot(hc.complete, main="Complete Linkage, euclidian", xlab="",sub ="")
plot(hc.single, main="Single Linkage, euclidian", xlab="",sub ="")
plot(hc.average, main="Average Linkage, euclidian", xlab="",sub ="")

```


## b)

<!-- Use these dendograms to cluster the tissues into two groups. Compare the groups with respect to the patient group the tissue comes from. Which linkage and distance measure performs best when we know the true state of the tissue? -->

As can be seen, the correlation based dendrogrammes are all over the place, with no clear way of classifying the samples into two classes. The euclidian distance based dendrograms however, are a different story. All of them classify the two classes perfectly, with no means of determining which one is the best. Usually, average or complete linking is better than single linking. 



## c) 

<!-- With Principal Component Analysis, the first principal component loading vector solves the following optimization problem, -->

\begin{equation*}
\max_{\phi_{11},...,\phi_{p1}} \Big\{ \frac{1}{n}\sum_{i=1}^n \Big( \sum_{j=1}^p \phi_{j1}x_{ij} \Big)^2  \Big\} \quad \text{subject to } \sum_{j=1}^p\phi_{j1}^2 = 1.
\end{equation*}

<!-- Explain what $\phi$, $p$, $n$ and $x$ are in this optimization problem and write down the formula for the first principal component scores. -->

$\phi$ is the weight, while $X$ is the variable. $P$ is the number of elements in the principal components, while $n$ is the number of samples, or more specifically, the number of principal components. 

The principal component score of the first principal component can be expressed as: $$z_1=\sum^{n}_{i=1}\sum^p_{j=1} \phi_{j1}X_{ij}$$

## d) 

### (i) 
<!-- (1P) Use PCA to plot the samples in two dimensions. Color the samples based on the tissues group of patients. -->

```{r}
pca = prcomp(GeneData, scale=TRUE)
biplot(pca, scale = 0, cex = 0.4)
```


```{r, fig.width=6, fig.height=3}
par(mfrow=c(1,2), cex = 0.6)
pca.var = pca$sdev^2
pve = pca.var/sum(pca.var)

plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained",
     ylim=c(0,1),type='b')
points(5,pve[5], col="black",cex=2,pch=20)
plot(cumsum(pve), xlab="Principal Component", 
     ylab="Cumulative Proportion of Variance Explained", 
     ylim=c(0,1),type='b')
points(5,cumsum(pve)[5], col="black",cex=2,pch=20)
print(cumsum(pca.var[1:5]))

```

### (ii) 
<!-- (1P) How much variance is explained by the first 5 PCs? -->
We can see that the 5 first principal components stand for 21% of the total variance. 


## e) 

<!-- Use your results from PCA to find which genes that vary the most across the two groups. -->

In order to find the genes that vary the most across one can perform PCA on the untransposed dataset GeneData. The Principal Components that have the most variance would be the genes that vary the most. 

```{r }
tr.genedata <- t(GeneData)
pca.gene = prcomp(tr.genedata, scale=TRUE)
gene <- pca.gene$rotation[,1]
gene_abs <- abs(gene)
gene_sorted <- sort(gene_abs, decreasing = TRUE)
print(names(gene_sorted[1:5]))

```
Thus, the 5 genes that contribute most variance to PC1 are; D8, D2, D19, D14 and D1.


## f) 

<!-- Use K-means to separate the tissue samples into two groups. Plot the values in a two-dimensional space with PCA. What is the error rate of K-means? -->
K-means was used to separate the tissue samples into two groups. 

```{r }
km <- kmeans(GeneData, 2, nstart=20)
plot(pca$x[,1:2], col=km$cluster, pca=c(GeneData[1:20], GeneData[21:40]))
```

The error rate of the K-means is zero. This can be seen by the perfect separation of the two groups. 

