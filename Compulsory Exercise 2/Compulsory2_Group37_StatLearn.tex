\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Compulsory Exercise 2: Group 37},
            pdfauthor={Anders Bendiksen and Helge Bergo},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother

\title{Compulsory Exercise 2: Group 37}
\providecommand{\subtitle}[1]{}
\subtitle{TMA4268 Statistical Learning V2019}
\author{Anders Bendiksen and Helge Bergo}
\date{20 March, 2020}

\begin{document}
\maketitle

\hypertarget{problem-1-10p}{%
\section{Problem 1 (10p)}\label{problem-1-10p}}

\hypertarget{a-ridge-regression-2p}{%
\subsection{a) Ridge Regression (2p)}\label{a-ridge-regression-2p}}

Show that the ridge regression estimator is
\(\hat\beta_{Ridge} = (X^T X + \lambda I)^{-1} X^T y\).

\hypertarget{b-2p}{%
\subsection{b) (2p)}\label{b-2p}}

Find the expected value and the variance-covariance matrix of
\(\hat\beta_{Ridge}\) (1P each).

\hypertarget{c-2p---multiple-choice}{%
\subsection{c) (2P) - Multiple choice}\label{c-2p---multiple-choice}}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  TRUE
\item
  FALSE
\item
  FALSE
\item
  TRUE
\end{enumerate}

\hypertarget{d-forward-selection}{%
\subsection{d) Forward Selection}\label{d-forward-selection}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ISLR)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{train.ind =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(College), }\FloatTok{0.5}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(College))}
\NormalTok{college.train =}\StringTok{ }\NormalTok{College[train.ind,]}
\NormalTok{college.test =}\StringTok{ }\NormalTok{College[}\OperatorTok{-}\NormalTok{train.ind,]}
\end{Highlighting}
\end{Shaded}

After dividing the data into a training and test set, the
\texttt{regsubsets} function was used to create a forward selection
model on the data, from the \texttt{leaps}-library.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(leaps)}
\NormalTok{regfit.fwd =}\StringTok{ }\KeywordTok{regsubsets}\NormalTok{(Outstate}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{college.train,}\DataTypeTok{method=}\StringTok{"forward"}\NormalTok{, }\DataTypeTok{nvmax =} \DecValTok{18}\NormalTok{)}
\NormalTok{reg.summary =}\StringTok{ }\KeywordTok{summary}\NormalTok{(regfit.fwd)}
\end{Highlighting}
\end{Shaded}

To decide on which model is best, the number of variables used in the
selection was plotted against \texttt{RSS}, \texttt{Cp}, \texttt{BIC}
and \texttt{adjusted\ R\$\^{}2\$}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{rss,}\DataTypeTok{xlab=}\StringTok{"Number of variables"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"RSS"}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{adjr2,}\DataTypeTok{xlab=}\StringTok{"Number of variables"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"Adjusted Rsq"}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{)}
\NormalTok{max.adjr2 =}\StringTok{ }\KeywordTok{which.max}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{adjr2)}
\KeywordTok{points}\NormalTok{(max.adjr2,reg.summary}\OperatorTok{$}\NormalTok{adjr2[max.adjr2], }\DataTypeTok{col=}\StringTok{"black"}\NormalTok{,}\DataTypeTok{cex=}\DecValTok{2}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{20}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{cp,}\DataTypeTok{xlab=}\StringTok{"Number of variables"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"Cp"}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{)}
\NormalTok{min.cp =}\StringTok{ }\KeywordTok{which.min}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{cp)}
\KeywordTok{points}\NormalTok{(min.cp,reg.summary}\OperatorTok{$}\NormalTok{cp[min.cp], }\DataTypeTok{col=}\StringTok{"black"}\NormalTok{,}\DataTypeTok{cex=}\DecValTok{2}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{20}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{bic,}\DataTypeTok{xlab=}\StringTok{"Number of variables"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"BIC"}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{)}
\NormalTok{min.bic =}\StringTok{ }\KeywordTok{which.min}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{bic)}
\KeywordTok{points}\NormalTok{(min.bic,reg.summary}\OperatorTok{$}\NormalTok{bic[min.bic], }\DataTypeTok{col=}\StringTok{"black"}\NormalTok{,}\DataTypeTok{cex=}\DecValTok{2}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Compulsory2_Group37_StatLearn_files/figure-latex/p2d3-1} \end{center}

The maximum \texttt{adjusted} \(R^2\) is the one with 14 variables, with
a value of 0.7706887, shown as a filled dot in the upper right plot.
This is also the same number of variables as for the lowest Cp. However,
all the plots are pretty flat after around 6 or 7 variables used, and it
seems like using only 6 variables still gives a good \texttt{adjusted}
\(R^2\) value of 0.7516133, without the increased complexity of adding 7
more variables. The model is then:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(regfit.fwd,}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   (Intercept)    PrivateYes    Room.Board      Terminal   perc.alumni 
## -4726.8810613  2717.7019276     1.1032433    36.9990286    59.0863753 
##        Expend     Grad.Rate 
##     0.1930814    33.8303314
\end{verbatim}

For the MSE, the following code calculates the MSE for all the
variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val.errors =}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{17}\NormalTok{)}
\NormalTok{x.test =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(Outstate}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{college.test) }\CommentTok{# notice the -index!}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{17}\NormalTok{) \{}
\NormalTok{    coefi =}\StringTok{ }\KeywordTok{coef}\NormalTok{(regfit.fwd,}\DataTypeTok{id=}\NormalTok{i)}
\NormalTok{    pred =}\StringTok{ }\NormalTok{x.test[,}\KeywordTok{names}\NormalTok{(coefi)]}\OperatorTok{%*%}\NormalTok{coefi}
\NormalTok{    val.errors[i] =}\StringTok{ }\KeywordTok{mean}\NormalTok{((college.test}\OperatorTok{$}\NormalTok{Outstate}\OperatorTok{-}\NormalTok{pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{# plot(sqrt(val.errors),xlab="Number of variables", ylab="Root MSE",ylim=c(1500,5000) ,pch=19,type="b")}
\CommentTok{# points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b")}
\CommentTok{# legend("topright",legend=c("Training","Validation"),col=c("black","blue"),pch=19)}
\end{Highlighting}
\end{Shaded}

The MSE of the model with 6 variables is then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val.errors[}\DecValTok{6}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3844857
\end{verbatim}

\hypertarget{e-2p}{%
\subsection{e) (2p)}\label{e-2p}}

Using the Lasso method from the \texttt{glmnet}-library, a new model was
selected.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(glmnet)}
\NormalTok{x.train =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(Outstate}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{college.train)[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\NormalTok{y.train =}\StringTok{ }\NormalTok{college.train}\OperatorTok{$}\NormalTok{Outstate}
\NormalTok{x.test =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(Outstate}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{college.test)[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\NormalTok{y.test =}\StringTok{ }\NormalTok{college.test}\OperatorTok{$}\NormalTok{Outstate}

\NormalTok{lasso.model =}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x.train,y.train,}\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(lasso.model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Compulsory2_Group37_StatLearn_files/figure-latex/unnamed-chunk-9-1} \end{center}

To select the tuning parameter \(\lambda\), cross-validation was
performed, and the \(\lambda\) giving the lowest MSE was selected.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.out =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(x.train,y.train, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(cv.out)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Compulsory2_Group37_StatLearn_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best.lambda =}\StringTok{ }\NormalTok{cv.out}\OperatorTok{$}\NormalTok{lambda.min}
\NormalTok{best.lambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10.7207
\end{verbatim}

This was used on the test set, to get the MSE for the

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(lasso.model,}\DataTypeTok{s=}\NormalTok{best.lambda ,}\DataTypeTok{newx=}\NormalTok{x.test)}
\NormalTok{MSE =}\StringTok{ }\KeywordTok{mean}\NormalTok{((lasso.pred}\OperatorTok{-}\NormalTok{y.test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3688061
\end{verbatim}

Finally, the coefficients of the model are shown here:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.coef =}\StringTok{ }\KeywordTok{predict}\NormalTok{(cv.out,}\DataTypeTok{type=}\StringTok{"coefficients"}\NormalTok{,}\DataTypeTok{s=}\NormalTok{best.lambda)[}\DecValTok{1}\OperatorTok{:}\DecValTok{18}\NormalTok{,]}
\NormalTok{lasso.coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   (Intercept)    PrivateYes          Apps        Accept        Enroll 
## -1.172140e+03  2.230467e+03 -2.825215e-01  6.615811e-01 -3.778631e-01 
##     Top10perc     Top25perc   F.Undergrad   P.Undergrad    Room.Board 
##  4.589180e+01 -1.485674e+01 -5.800132e-02 -5.713770e-02  1.088115e+00 
##         Books      Personal           PhD      Terminal     S.F.Ratio 
## -9.185125e-01 -3.005419e-01  4.013410e+00  2.996744e+01 -6.936391e+01 
##   perc.alumni        Expend     Grad.Rate 
##  4.686967e+01  1.480013e-01  2.431539e+01
\end{verbatim}

\hypertarget{problem-2-9p}{%
\section{Problem 2 (9p)}\label{problem-2-9p}}

\hypertarget{a-2p---multiple-choice}{%
\subsection{a) (2p) - Multiple choice}\label{a-2p---multiple-choice}}

Which of the following statements are true, which false?

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  A regression spline of order 3 with 4 knots has 8 basis functions.
\item
  A regression spline with polynomials of degree \(M-1\) has continous
  derivatives up to order \(M-2,\) but not at the knots.
\item
  A natural cubic spline is linear beyond the boundary knots.
\item
  A smoothing spline is (a shrunken version of) a natural cubic spline
  with knots at the values of all data points \(x_i\) for
  \(i=1,\ldots ,n\).
\item
\item
\item
\item
\end{enumerate}

\hypertarget{b-2p-1}{%
\subsection{b) (2p)}\label{b-2p-1}}

Write down the basis functions for a cubic spline with knots at the
quartiles \(q_1, q_2, q_3\) of variable \(X\).

\hypertarget{c-2p}{%
\subsection{c) (2p)}\label{c-2p}}

We continue with using the \texttt{College} dataset that we used in
problem 1. Investigate the relationships between \texttt{Outstate} and
the following 6 predictors (using the training dataset
\texttt{college.train}):

\begin{verbatim}
## [1] "Private"     "Room.Board"  "Terminal"    "perc.alumni" "Expend"     
## [6] "Grad.Rate"
\end{verbatim}

Create some informative plots and say which of the variables seem to
have a linear relationship and which not and might thus benefit from a
non-linear transformation (like e.g.~a spline).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(College}\OperatorTok{$}\NormalTok{Private,College}\OperatorTok{$}\NormalTok{Outstate)}
\KeywordTok{plot}\NormalTok{(College}\OperatorTok{$}\NormalTok{Room.Board,College}\OperatorTok{$}\NormalTok{Outstate)}
\KeywordTok{plot}\NormalTok{(College}\OperatorTok{$}\NormalTok{Terminal,College}\OperatorTok{$}\NormalTok{Outstate)}
\KeywordTok{plot}\NormalTok{(College}\OperatorTok{$}\NormalTok{perc.alumni,College}\OperatorTok{$}\NormalTok{Outstate)}
\KeywordTok{plot}\NormalTok{(College}\OperatorTok{$}\NormalTok{Expend,College}\OperatorTok{$}\NormalTok{Outstate)}
\KeywordTok{plot}\NormalTok{(College}\OperatorTok{$}\NormalTok{Grad.Rate,College}\OperatorTok{$}\NormalTok{Outstate)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Compulsory2_Group37_StatLearn_files/figure-latex/2c3-1} \end{center}

From these plots, it seems like \texttt{Room.board},
\texttt{perc.alumni} and \texttt{Grad.Rate} all have quite linear
relationshps with \texttt{Outstate}, while both \texttt{Terminal} and
\texttt{Expend} seem to follow a non-linear relationship.

\hypertarget{d-3p}{%
\subsection{d) (3P)}\label{d-3p}}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Fit polynomial regression models for \texttt{Outstate} with
  \texttt{Terminal} as the only covariate for a range of polynomial
  degrees (\(d = 1,\ldots,10\)) and plot the results. Use the training
  data (\texttt{college.train}) for this task.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{) \{}
\NormalTok{  poly.fit =}\StringTok{ }\KeywordTok{lm}\NormalTok{(Outstate }\OperatorTok{~}\StringTok{ }\KeywordTok{poly}\NormalTok{(Terminal,i), }\DataTypeTok{data=}\NormalTok{college.train)}
  \CommentTok{# plot(poly.fit)  }
\NormalTok{\}}
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(poly.fit))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                       Estimate Std. Error    t value      Pr(>|t|)
## (Intercept)         10484.0000   191.4776 54.7531395 1.592465e-181
## poly(Terminal, i)1  33775.7920  3771.6714  8.9551258  1.576072e-17
## poly(Terminal, i)2  16996.9239  3771.6714  4.5064700  8.807400e-06
## poly(Terminal, i)3   5610.8187  3771.6714  1.4876213  1.376867e-01
## poly(Terminal, i)4    906.6817  3771.6714  0.2403925  8.101566e-01
## poly(Terminal, i)5  -2479.4660  3771.6714 -0.6573918  5.113301e-01
## poly(Terminal, i)6    651.1277  3771.6714  0.1726364  8.630299e-01
## poly(Terminal, i)7  -5472.4748  3771.6714 -1.4509416  1.476277e-01
## poly(Terminal, i)8  -4631.9185  3771.6714 -1.2280811  2.201827e-01
## poly(Terminal, i)9  -9973.8438  3771.6714 -2.6444095  8.525278e-03
## poly(Terminal, i)10 -2737.9707  3771.6714 -0.7259303  4.683319e-01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ISLR)}
\CommentTok{# extract only the two variables from Auto}
\NormalTok{ds =}\StringTok{ }\NormalTok{Auto[}\KeywordTok{c}\NormalTok{(}\StringTok{"horsepower"}\NormalTok{, }\StringTok{"mpg"}\NormalTok{)]}
\NormalTok{ds =}\StringTok{ }\NormalTok{college.train[}\KeywordTok{c}\NormalTok{(}\StringTok{"Terminal"}\NormalTok{,}\StringTok{"Outstate"}\NormalTok{)]}
\NormalTok{n =}\StringTok{ }\KeywordTok{nrow}\NormalTok{(ds)}

\CommentTok{# which degrees we will look at}
\NormalTok{deg =}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{10}

\CommentTok{# training ids for training set}
\NormalTok{tr =}\StringTok{ }\KeywordTok{sample.int}\NormalTok{(n, n}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\CommentTok{# plot of training data}
\CommentTok{# plot(ds[tr, ], col = "darkgrey", main = "Polynomial regression")}
\KeywordTok{plot}\NormalTok{(college.train}\OperatorTok{$}\NormalTok{Outstate,}\DataTypeTok{col =} \StringTok{"darkgrey"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Polynomial regression"}\NormalTok{)}

\CommentTok{# which colors we will plot the lines with}
\NormalTok{colors =}\StringTok{ }\KeywordTok{rainbow}\NormalTok{(}\KeywordTok{length}\NormalTok{(deg))}
\CommentTok{# iterate over all degrees (1:4) - could also use a for-loop here }
\NormalTok{MSE =}\StringTok{ }\KeywordTok{sapply}\NormalTok{(deg, }\ControlFlowTok{function}\NormalTok{(d) \{}
  \CommentTok{# fit model with this degree}
\NormalTok{  model =}\StringTok{ }\KeywordTok{lm}\NormalTok{(Outstate }\OperatorTok{~}\StringTok{ }\KeywordTok{poly}\NormalTok{(Terminal, d), }\DataTypeTok{data=}\NormalTok{college.train)}
  
  \KeywordTok{lines}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(college.train}\OperatorTok{$}\NormalTok{Terminal, model}\OperatorTok{$}\NormalTok{fit)[}\KeywordTok{order}\NormalTok{(college.train}\OperatorTok{$}\NormalTok{Terminal) ,], }\DataTypeTok{col =}\NormalTok{ colors[d])}
  \CommentTok{# add lines to the plot - use fitted values (for mpg) and horsepower from # training set}
  \CommentTok{# lines(cbind(ds[tr, 1], model$fit)[order(ds[tr, 1]), ], col = colors[d])}
  
  \CommentTok{# calculate mean MSE - this is returned in the MSE variable }
  \CommentTok{# mean((predict(mod, ds[-tr, ]) - ds[-tr, 2])^2)}
\NormalTok{\})}
\CommentTok{# add legend to see which color corresponds to which line}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\DataTypeTok{legend =} \KeywordTok{paste}\NormalTok{(}\StringTok{"d ="}\NormalTok{, deg), }\DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }\DataTypeTok{col =}\NormalTok{ colors)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Compulsory2_Group37_StatLearn_files/figure-latex/unnamed-chunk-16-1} \end{center}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Still for the training data, choose a suitable smoothing spline model
  to predict \texttt{Outstate} as a function of \texttt{Expend} (again
  as the only covariate) and plot the fitted function into the
  scatterplot of \texttt{Outstate} against \texttt{Expend}. How did you
  choose the degrees of freedom?
\item
  Report the corresponding training MSE for (i) and (ii). Did you expect
  that?
\end{enumerate}

\hypertarget{problem-3-9p}{%
\section{Problem 3 (9p)}\label{problem-3-9p}}

\hypertarget{a-2p---multiple-choice-1}{%
\subsection{a) (2P) - Multiple choice}\label{a-2p---multiple-choice-1}}

Which of the following statements are true, which false?

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Regression trees cannot handle categorical predictors.
\item
  Regression and classification trees are easy to interpret.
\item
  The random forest approaches improves bagging, because it reduces the
  variance of the predictor function by decorrelating the trees.
\item
  The number of trees \(B\) in bagging and random forests is a tuning
  parameter.
\end{enumerate}

\hypertarget{b-4p}{%
\subsection{b) (4P)}\label{b-4p}}

Select one method from Module 8 (tree-based methods) in order to build a
good model to predict \texttt{Outstate} in the \texttt{College} dataset
that we used in problems 1 and 2. Explain your choice (pros/cons?) and
how you chose the tuning parameter(s). Train the model using the
training data and report the MSE for the test data.

\hypertarget{c-2p-1}{%
\subsection{c) (2p)}\label{c-2p-1}}

Compare the results (tests MSEs) among all the methods you used in
Problems 1-3. Which method perform best in terms of prediction error?
Which method would you choose if the aim is to develop an interpretable
model?

\hypertarget{problem-4-12p}{%
\section{Problem 4 (12P)}\label{problem-4-12p}}

We will use the classical data set of \emph{diabetes} from a population
of women of Pima Indian heritage in the US, available in the R
\texttt{MASS} package. The following information is available for each
woman:

\begin{itemize}
\tightlist
\item
  diabetes: \texttt{0}= not present, \texttt{1}= present
\item
  npreg: number of pregnancies
\item
  glu: plasma glucose concentration in an oral glucose tolerance test
\item
  bp: diastolic blood pressure (mmHg)
\item
  skin: triceps skin fold thickness (mm)
\item
  bmi: body mass index (weight in kg/(height in m)\(^2\))
\item
  ped: diabetes pedigree function.
\item
  age: age in years
\end{itemize}

We will use a training set (called \texttt{d.train}) with 300
observations (200 non-diabetes and 100 diabetes cases) and a test set
(called \texttt{d.test}) with 232 observations (155 non-diabetes and 77
diabetes cases). Our aim is to make a classification rule for the
presence of diabetes (yes/no) based on the available data. You can load
the data as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{id <-}\StringTok{ "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E"} \CommentTok{# google file ID}
\NormalTok{d.diabetes <-}\StringTok{ }\KeywordTok{dget}\NormalTok{(}\KeywordTok{sprintf}\NormalTok{(}\StringTok{"https://docs.google.com/uc?id=%s&export=download"}\NormalTok{, id))}
\NormalTok{d.train=d.diabetes}\OperatorTok{$}\NormalTok{ctrain}
\NormalTok{d.test=d.diabetes}\OperatorTok{$}\NormalTok{ctest}
\end{Highlighting}
\end{Shaded}

\hypertarget{a-2p---multiple-choice-2}{%
\subsection{a) (2P) - Multiple choice}\label{a-2p---multiple-choice-2}}

Start by getting to know the \emph{training data}, by producing
summaries and plots. Which of the following statements are true, which
false?

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Females with high glucose levels and higher bmi seem to have a higher
  risk for diabetes.
\item
  Some women had up to 17 pregnancies.
\item
  BMI and triceps skin fold thickness seem to be positively correlated.
\item
  The distribution of the number of pregnancies per woman seems to be a
  bit skewed and a transformation of this variable could therefore be
  appropriate.
\end{enumerate}

\hypertarget{b-4p-1}{%
\subsection{b) (4P)}\label{b-4p-1}}

Fit a support vector classifier (linear boundary) and a support vector
machine (radial boundary) to find good functions that predict the
diabetes status of a patient. Use cross-validation to find a good cost
parameter (for the linear boundary) and a good combination of cost
\emph{and} \(\gamma\) parameters (for the radial boundary). Report the
confusion tables and misclassification error rates for the test set in
both cases. Which classifier do you prefer and why? (Do not use any
variable transformations or standardizations to facilitate correction).

\textbf{R-hints:} The response variable must be converted into a factor
variable before you continue.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d.train}\OperatorTok{$}\NormalTok{diabetes <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(d.train}\OperatorTok{$}\NormalTok{diabetes)}
\NormalTok{d.test}\OperatorTok{$}\NormalTok{diabetes <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(d.test}\OperatorTok{$}\NormalTok{diabetes)}
\end{Highlighting}
\end{Shaded}

To run cross-validation over a grid of two tuning parameters, you can
use the \texttt{tune()} function where \texttt{ranges} defines the grid
points as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tune}\NormalTok{(..., formula, }\DataTypeTok{kernel=}\NormalTok{...,}\DataTypeTok{ranges=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{cost=}\KeywordTok{c}\NormalTok{(...), }\DataTypeTok{gamma=}\KeywordTok{c}\NormalTok{(...)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{c-2p-2}{%
\subsection{c) (2P)}\label{c-2p-2}}

Compare the performance of the two classifiers from b) to \emph{one
other classification method} that you have learned about in the course.
Explain your choice and report the confusion table and misclassification
error rate on the test set for your chosen method and interpret what you
see. What are advantages/disadvantages of your chosen method with
respect to SVMs?

\hypertarget{d-2p---multiple-choice}{%
\subsection{d) (2P) - Multiple choice}\label{d-2p---multiple-choice}}

Which of the following statements are true, which false?

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Under standard conditions, the maximal margin hyperplane approach is
  equivalent to a linear discriminant analysis.
\item
  Under standard conditions, the support vector classifier is equivalent
  to quadratic discriminant analysis.
\item
  Logistic regression, LDA and support vector machines tend to perform
  similar when decision boundaries are linear, unless classes are
  linearly separable.
\item
  An advantage of logistic regression over SVMs is that it is easier to
  do feature selection and to interpret the results.
\end{enumerate}

\hypertarget{e-2p-link-to-logistic-regression-and-hinge-loss.}{%
\subsection{e) (2P) Link to logistic regression and hinge
loss.}\label{e-2p-link-to-logistic-regression-and-hinge-loss.}}

Look at slides 71-73 of Module 9. Show that the loss function
\[ \log(1+\exp(-y_i f({\boldsymbol x}_i)))\]

is the deviance for the \(y=-1,1\) encoding in a logistic regression
model.\\
\textbf{Hint}: \(f({\boldsymbol x}_i)\) correponds to the linear
predictor in the logistic regression approach.

\hypertarget{problem-5-10p}{%
\section{Problem 5 (10P)}\label{problem-5-10p}}

The following dataset consists of 40 tissue samples with measurements of
1,000 genes. The first 20 tissues come from healthy patients and the
remaining 20 come from a diseased patient group. The following code
loads the dataset into your session with row names decribing if the
tissue comes from a diseased or healthy person.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU" # google file ID}
\CommentTok{# GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=F)}
\CommentTok{# colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")}
\CommentTok{# colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")}
\CommentTok{# # row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")}
\end{Highlighting}
\end{Shaded}

\hypertarget{a-2p}{%
\subsection{a) (2P)}\label{a-2p}}

Perform hierarchical clustering with complete, single and average
linkage using \textbf{both} Euclidean distance and correlation-based
distance on the dataset. Plot the dendograms. Hint: You can use
\texttt{par(mfrow=c(1,3))} to plot all three dendograms on one line or
\texttt{par(mfrow=c(2,3))} to plot all six together.

\hypertarget{b-2p-2}{%
\subsection{b) (2P)}\label{b-2p-2}}

Use these dendograms to cluster the tissues into two groups. Compare the
groups with respect to the patient group the tissue comes from. Which
linkage and distance measure performs best when we know the true state
of the tissue?

\hypertarget{c-1p}{%
\subsection{c) (1P)}\label{c-1p}}

With Principal Component Analysis, the first principal component loading
vector solves the following optimization problem,

\begin{equation*}
\max_{\phi_{11},...,\phi_{p1}} \Big\{ \frac{1}{n}\sum_{i=1}^n \Big( \sum_{j=1}^p \phi_{j1}x_{ij} \Big)^2  \Big\} \quad \text{subject to } \sum_{j=1}^p\phi_{j1}^2 = 1.
\end{equation*}

Explain what \(\phi\), \(p\), \(n\) and \(x\) are in this optimization
probelm and write down the formula for the first principal component
scores.

\hypertarget{d-2p}{%
\subsection{d) (2P)}\label{d-2p}}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  (1P) Use PCA to plot the samples in two dimensions. Color the samples
  based on the tissues group of patients.
\item
  (1P) How much variance is explained by the first 5 PCs?
\end{enumerate}

\hypertarget{e-1p}{%
\subsection{e) (1P)}\label{e-1p}}

Use your results from PCA to find which genes that vary the most accross
the two groups.

\hypertarget{f-2p}{%
\subsection{f) (2P)}\label{f-2p}}

Use K-means to seperate the tissue samples into two groups. Plot the
values in a two-dimensional space with PCA. What is the error rate of
K-means?

\end{document}
